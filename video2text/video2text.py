#!/usr/bin/env python3
# file: video2text.py
# convierte video.mp4 â”€> video.txt y video.srt usando Whisper (todo local)

from pathlib import Path
import whisper
import sys

# --- ConfiguraciÃ³n rÃ¡pida ----------------------------------------------------
VIDEO_FILE = Path(__file__).with_name("video6.mp4")   # raÃ­z del proyecto
MODEL_SIZE = "large"          # usa "small" / "medium" / "large" si necesitas
LANG       = "es"            # ajusta si el video no estÃ¡ en espaÃ±ol
USE_FP16   = True         # pon True si tienes GPU con soporte FP16
DEVICE   = "cuda"
# -----------------------------------------------------------------------------

if not VIDEO_FILE.exists():
    sys.exit("âŒ  No se encontrÃ³ video.mp4 en la raÃ­z del proyecto.")

print("â³  Cargando modelo Whisperâ€¦")
model = whisper.load_model(MODEL_SIZE)

print(f"ğŸ™ï¸  Transcribiendo {VIDEO_FILE.name} â€¦")
result = model.transcribe(
    str(VIDEO_FILE),
    language=LANG,
    fp16=USE_FP16,
    word_timestamps=False,   # pon True si quieres tiempo por palabra
)

# --- Guarda texto plano ------------------------------------------------------
text_file = VIDEO_FILE.with_suffix(".txt")
text_file.write_text(result["text"], encoding="utf-8")
print(f"âœ…  TranscripciÃ³n guardada en {text_file}")

# --- Guarda subtÃ­tulos SRT ----------------------------------------------------
def srt_timestamp(seconds: float) -> str:
    h, m = divmod(int(seconds), 3600)
    m, s = divmod(m, 60)
    ms   = int((seconds - int(seconds)) * 1000)
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

srt_lines = []
for idx, seg in enumerate(result["segments"], start=1):
    start, end = seg["start"], seg["end"]
    srt_lines.append(f"{idx}")
    srt_lines.append(f"{srt_timestamp(start)} --> {srt_timestamp(end)}")
    srt_lines.append(seg["text"].strip())
    srt_lines.append("")  # lÃ­nea en blanco

srt_file = VIDEO_FILE.with_suffix(".srt")
srt_file.write_text("\n".join(srt_lines), encoding="utf-8")
print(f"ğŸï¸  SubtÃ­tulos SRT guardados en {srt_file}")

print("ğŸ  Terminado.")
